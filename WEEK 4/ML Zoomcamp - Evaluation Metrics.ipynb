{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.feature_extraction import DictVectorizer\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('../WEEK 3/data-week-3.csv')\r\n",
    "\r\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\r\n",
    "\r\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\r\n",
    "\r\n",
    "for c in categorical_columns:\r\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\r\n",
    "\r\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\r\n",
    "df.totalcharges = df.totalcharges.fillna(0)\r\n",
    "\r\n",
    "df.churn = (df.churn == 'yes').astype(int)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\r\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\r\n",
    "\r\n",
    "df_train = df_train.reset_index(drop=True)\r\n",
    "df_val = df_val.reset_index(drop=True)\r\n",
    "df_test = df_test.reset_index(drop=True)\r\n",
    "\r\n",
    "y_train = df_train.churn.values\r\n",
    "y_val = df_val.churn.values\r\n",
    "y_test = df_test.churn.values\r\n",
    "\r\n",
    "del df_train['churn']\r\n",
    "del df_val['churn']\r\n",
    "del df_test['churn']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\r\n",
    "\r\n",
    "categorical = [\r\n",
    "    'gender',\r\n",
    "    'seniorcitizen',\r\n",
    "    'partner',\r\n",
    "    'dependents',\r\n",
    "    'phoneservice',\r\n",
    "    'multiplelines',\r\n",
    "    'internetservice',\r\n",
    "    'onlinesecurity',\r\n",
    "    'onlinebackup',\r\n",
    "    'deviceprotection',\r\n",
    "    'techsupport',\r\n",
    "    'streamingtv',\r\n",
    "    'streamingmovies',\r\n",
    "    'contract',\r\n",
    "    'paperlessbilling',\r\n",
    "    'paymentmethod',\r\n",
    "]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dv = DictVectorizer(sparse=False)\r\n",
    "\r\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\r\n",
    "X_train = dv.fit_transform(train_dict)\r\n",
    "\r\n",
    "model = LogisticRegression()\r\n",
    "model.fit(X_train, y_train)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\r\n",
    "X_val = dv.transform(val_dict)\r\n",
    "\r\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\r\n",
    "churn_decision = (y_pred >= 0.5)\r\n",
    "(y_val == churn_decision).mean()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "t=0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "accuracy_score(y_val,y_pred >= t)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "thresholds = np.linspace(0,1,21)\r\n",
    "\r\n",
    "scores = []\r\n",
    "\r\n",
    "for t in thresholds:\r\n",
    "    score = accuracy_score(y_val,y_pred >= t)\r\n",
    "    print(\"%.2f %.3f\"%(t,score))\r\n",
    "    scores.append(score) # append to scores array\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(thresholds,scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter # Counting yes/no chourning \r\n",
    "Counter(y_val)\r\n",
    "\r\n",
    "# Class imbalance (much more true values than false ones)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "actual_positive = (y_val ==1 )\r\n",
    "actual_negative = (y_val ==0 )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "threshold = 0.5\r\n",
    "\r\n",
    "predict_positive = (y_pred >= threshold)\r\n",
    "predict_negative = (y_pred < threshold)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = Counter(predict_positive & actual_positive)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize =(10, 7))\r\n",
    "plt.pie([float(v) for v in results.values()], labels = ['False','True'],pctdistance=1.1, labeldistance=1.2,autopct='%1.0f%%')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating true positives and true negatives\r\n",
    "tp = (predict_positive & actual_positive).sum()\r\n",
    "tn = (predict_negative & actual_negative).sum()\r\n",
    "\r\n",
    "# False positives and false negatives\r\n",
    "fp = (predict_positive & actual_negative).sum()\r\n",
    "fn = (predict_negative & actual_positive).sum()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fp,fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating the confusion matrix\r\n",
    "confusion_matrix = np.array([\r\n",
    "    [tn,fp],\r\n",
    "    [fn,tp]\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "( confusion_matrix / confusion_matrix.sum() ).round(2) # PORCENTAGE OF CORRECT "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(tp + tn) / ( tp + tn + fp + fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.4 precision and Recall"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision\r\n",
    "- Fraction of positive predictions that are correct.\r\n",
    "\r\n",
    "$$P=\\frac{TP}{TP+FP}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## Recall\r\n",
    "- Fraction of correctly identified positive examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- We will only use the $g(x_i) \\geq t$ (FN) and y=1 (TP) part of the data set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$R= \\frac{TP}{ \\# Positive (OBS)} =\\frac{TP}{TP+FN} $$\r\n",
    "$$R= \\frac{3}{4} = 75 \\%$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall = tp/ (tp + fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tp + fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.5 ROC CURVES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ FPR = \\frac{FP}{TN+FP} $$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$TPR= \\frac{TP}{FN+TP}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Minimize FP\r\n",
    "- Max TP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tpr = tp/(tp+fn)\r\n",
    "tpr # same as recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fpr = fp/(fp+tn)\r\n",
    "fpr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores = []\r\n",
    "\r\n",
    "thresholds = np.linspace(0,1,101)\r\n",
    "\r\n",
    "for t in thresholds:\r\n",
    "    actual_positive = (y_val ==1 )\r\n",
    "\r\n",
    "    actual_negative = (y_val ==0 )\r\n",
    "    \r\n",
    "    predict_positive = (y_pred >= t)\r\n",
    "    predict_negative = (y_pred < t)\r\n",
    "\r\n",
    "\r\n",
    "    # Creating true positives and true negatives\r\n",
    "    tp = (predict_positive & actual_positive).sum()\r\n",
    "    tn = (predict_negative & actual_negative).sum()\r\n",
    "\r\n",
    "    # False positives and false negatives\r\n",
    "    fp = (predict_positive & actual_negative).sum()\r\n",
    "    fn = (predict_negative & actual_positive).sum()\r\n",
    "    #append tuple of true positives and false negatives\r\n",
    "\r\n",
    "    scores.append((t,tp,fp,fn,tn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns = ['threshold', 'tp','fp','fn','tn']\r\n",
    "df_scores = pd.DataFrame(scores,columns=columns) # Creating a data frame with pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\r\n",
    "df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\r\n",
    "\r\n",
    "df_scores[::10] # Each tenth record"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Ploting the results\r\n",
    "\r\n",
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR',color=\"green\")\r\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR',color=\"red\")\r\n",
    "plt.legend()\r\n",
    "\r\n",
    "# Minimize FPR , Maximize TPR"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(1)\r\n",
    "y_rand = np.random.uniform(0,1,size=len(y_val))\r\n",
    "\r\n",
    "y_rand.round(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "((y_rand >= 0.5) == y_val).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tpr_fpr_dataframe(y_val,y_pred):\r\n",
    "    scores = []\r\n",
    "    thresholds = np.linspace(0,1,101)\r\n",
    "    for t in thresholds:\r\n",
    "        actual_positive = (y_val ==1 )\r\n",
    "        actual_negative = (y_val ==0 )\r\n",
    "        predict_positive = (y_pred >= t)\r\n",
    "        predict_negative = (y_pred < t)\r\n",
    "\r\n",
    "        # Creating true positives and true negatives\r\n",
    "        tp = (predict_positive & actual_positive).sum()\r\n",
    "        tn = (predict_negative & actual_negative).sum()\r\n",
    "\r\n",
    "        # False positives and false negatives\r\n",
    "        fp = (predict_positive & actual_negative).sum()\r\n",
    "        fn = (predict_negative & actual_positive).sum()\r\n",
    "        #append tuple of true positives and false negatives\r\n",
    "\r\n",
    "        scores.append((t,tp,fp,fn,tn))\r\n",
    "    columns = ['threshold', 'tp','fp','fn','tn']\r\n",
    "    df_scores = pd.DataFrame(scores,columns=columns) # Creating a data frame with pandas    \r\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\r\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\r\n",
    "\r\n",
    "    return df_scores\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_rand = tpr_fpr_dataframe(y_val,y_rand)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_rand[::10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(df_scores.threshold, df_rand['tpr'], label='TPR',color=\"green\")\r\n",
    "plt.plot(df_scores.threshold, df_rand['fpr'], label='FPR',color=\"red\")\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_neg = (y_val == 0 ).sum()\r\n",
    "num_pos = (y_val == 1 ).sum()\r\n",
    "num_pos,num_neg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_ideal=np.repeat([0,1],[num_neg,num_pos])\r\n",
    "\r\n",
    "y_ideal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_ideal_pred = np.linspace(0,1,len(y_val))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "( (y_ideal_pred >= 0.726) == y_ideal ).mean() # perfect model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_ideal = tpr_fpr_dataframe(y_ideal,y_ideal_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_ideal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\r\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\r\n",
    "\r\n",
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\r\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\r\n",
    "\r\n",
    "# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\r\n",
    "# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\r\n",
    "\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(5,5))\r\n",
    "\r\n",
    "plt.plot(df_scores.fpr,df_scores.tpr, label='model')\r\n",
    "plt.plot([0,1],[0,1], label='random')\r\n",
    "plt.plot(df_ideal.fpr,df_ideal.tpr, label='ideal')\r\n",
    "\r\n",
    "plt.xlabel('FPR')\r\n",
    "plt.ylabel('TPR')\r\n",
    "\r\n",
    "\r\n",
    "print(\"Curve should be as close as possible to the ideal model\")\r\n",
    "\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve\r\n",
    "\r\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\r\n",
    "\r\n",
    "plt.figure(figsize=(5, 5))\r\n",
    "\r\n",
    "plt.plot(fpr, tpr, label='Model')\r\n",
    "plt.plot([0, 1], [0, 1], label='Random', linestyle='--')\r\n",
    "\r\n",
    "plt.xlabel('FPR')\r\n",
    "plt.ylabel('TPR')\r\n",
    "\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import auc # for any curve \r\n",
    "\r\n",
    "auc(fpr,tpr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auc(df_ideal.fpr,df_ideal.tpr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score\r\n",
    "\r\n",
    "roc_auc_score(y_val,y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random \r\n",
    "\r\n",
    "neg = y_pred[y_val == 0 ]\r\n",
    "pos = y_pred[y_val == 1 ]\r\n",
    "\r\n",
    "n= 100000\r\n",
    "success= 0\r\n",
    "\r\n",
    "for i in range(n):\r\n",
    "    pos_ind = random.randint(0,len(pos)-1)\r\n",
    "    neg_ind = random.randint(0,len(neg)-1)\r\n",
    "    if pos[pos_ind] > neg[neg_ind]:\r\n",
    "        success= success + 1\r\n",
    "\r\n",
    "\r\n",
    "success/n\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n= 50000\r\n",
    "np.random.seed(1)\r\n",
    "pos_ind = np.random.randint(0,len(pos),size=n)\r\n",
    "neg_ind = np.random.randint(0,len(neg),size=n)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()\r\n",
    "\r\n",
    "Counter(pos[pos_ind] > neg[neg_ind])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross-Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Splitting the data into 2 parts ( full_train and test )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "def train(df_train, y_train, C=1.0):\r\n",
    "\r\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\r\n",
    "    dv= DictVectorizer(sparse=False)\r\n",
    "    X_train = dv.fit_transform(dicts)\r\n",
    "    model = LogisticRegression(C=C ,max_iter=1000) # adding parameter\r\n",
    "    model.fit(X_train,y_train)\r\n",
    "    return dv,model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dv,model = train(df_train,y_train,C=0.0001) # Smaller C means stronger Regularization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict(df,dv,model):\r\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\r\n",
    "    X = dv.transform(dicts)\r\n",
    "    y_pred = model.predict_proba(X)[:,1]\r\n",
    "    return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = predict(df_val,dv,model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import KFold\r\n",
    "\r\n",
    "# take the data set,  and splitting it into 10 parts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kfold = KFold(n_splits=10,shuffle=True,random_state=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_idx,val_idx= next(kfold.split(df_full_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(train_idx),len(val_idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(df_full_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train = df_full_train.iloc[train_idx]\r\n",
    "df_val = df_full_train.iloc[val_idx]\r\n",
    "\r\n",
    "\r\n",
    "n_splits=5\r\n",
    "\r\n",
    "\r\n",
    "for C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\r\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\r\n",
    "    scores= []\r\n",
    "    for train_idx,val_idx in kfold.split(df_full_train):\r\n",
    "\r\n",
    "        df_train = df_full_train.iloc[train_idx]\r\n",
    "        df_val = df_full_train.iloc[val_idx]\r\n",
    "        y_train = df_train.churn.values\r\n",
    "        y_val = df_val.churn.values\r\n",
    "        dv,model = train(df_train,y_train,C=C)\r\n",
    "        y_pred = predict(df_val,dv,model)\r\n",
    "        auc= roc_auc_score(y_val,y_pred)    \r\n",
    "        scores.append(auc)\r\n",
    "    print(\"C=%s %.3f + - %.3f \" %(C,np.mean(scores),np.std(scores)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\r\n",
    "y_pred = predict(df_test, dv, model)\r\n",
    "\r\n",
    "auc = roc_auc_score(y_test, y_pred)\r\n",
    "auc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8572386167896259"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "df_full_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5634, 21)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "df_full_train.churn.values.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5634,)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "round(np.mean(scores),3),round(np.std(scores),3)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.841, 0.007)"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "468f25ad0239460415b7e6b7483d5c8f7213894121f6fb96c4cb6ef93fffe534"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}